name: Quality Gate

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]

# Ensure only one quality gate runs at a time per PR/branch
concurrency:
  group: quality-gate-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Fast preliminary checks
  preliminary-checks:
    name: Preliminary Checks
    runs-on: ubuntu-latest
    timeout-minutes: 3
    outputs:
      has-python-changes: ${{ steps.changes.outputs.python }}
      has-test-changes: ${{ steps.changes.outputs.tests }}
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Check for relevant changes
      uses: dorny/paths-filter@v2
      id: changes
      with:
        filters: |
          python:
            - '**/*.py'
            - 'basedpyrightconfig.json'
            - 'requirements.txt'
          tests:
            - 'tests/**/*.py'

  # Type checking (runs independently)
  type-check:
    name: Type Check
    needs: preliminary-checks
    if: needs.preliminary-checks.outputs.has-python-changes == 'true'
    uses: ./.github/workflows/type-check.yml
    
  # Mock density check (runs independently)  
  mock-density:
    name: Mock Density Check
    needs: preliminary-checks
    if: needs.preliminary-checks.outputs.has-test-changes == 'true'
    uses: ./.github/workflows/mock-density.yml

  # Lint and basic code quality (fast checks)
  code-quality:
    name: Code Quality
    needs: preliminary-checks
    if: needs.preliminary-checks.outputs.has-python-changes == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install ruff
        
    - name: Run ruff linting
      run: |
        ruff check . --output-format=github
        
    - name: Check import sorting
      run: |
        ruff check . --select I --diff

  # Integration and final quality assessment
  quality-assessment:
    name: Quality Assessment
    needs: [type-check, mock-density, code-quality]
    if: always() && (needs.preliminary-checks.outputs.has-python-changes == 'true' || needs.preliminary-checks.outputs.has-test-changes == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download type check results
      uses: actions/download-artifact@v3
      if: needs.type-check.result != 'skipped'
      with:
        name: type-check-results
        path: quality-results/type-check/
        
    - name: Download mock density results
      uses: actions/download-artifact@v3
      if: needs.mock-density.result != 'skipped'
      with:
        name: mock-density-results
        path: quality-results/mock-density/
        
    - name: Generate comprehensive quality report
      run: |
        mkdir -p quality-results/final
        
        # Create the generate_quality_report.py script if it doesn't exist
        cat > scripts/generate_quality_report.py << 'EOF'
        #!/usr/bin/env python3
        """
        Quality Report Generator
        
        Combines results from various quality checks into comprehensive reports.
        """
        
        import argparse
        import json
        import os
        from datetime import datetime
        from pathlib import Path
        from typing import Dict, Any, List
        
        
        def load_json_report(file_path: str) -> Dict[str, Any]:
            """Load a JSON report file safely."""
            try:
                if os.path.exists(file_path):
                    with open(file_path) as f:
                        return json.load(f)
                return {}
            except Exception as e:
                print(f"Warning: Could not load {file_path}: {e}")
                return {}
        
        
        def calculate_quality_score(type_check_data: Dict, mock_density_data: Dict) -> Dict[str, Any]:
            """Calculate overall quality score based on metrics."""
            score = 100  # Start with perfect score
            
            # Type checking penalties
            type_errors = type_check_data.get('total_errors', 0)
            critical_errors = type_check_data.get('critical_errors', 0)
            
            score -= critical_errors * 2  # Critical errors cost more
            score -= type_errors * 0.5    # Regular errors cost less
            
            # Mock density penalties
            mock_violations = len(mock_density_data.get('violations', []))
            avg_density = mock_density_data.get('summary', {}).get('avg_density', 0)
            
            score -= mock_violations * 5   # Each violation costs 5 points
            score -= avg_density * 100     # High average density is bad
            
            score = max(0, min(100, score))  # Clamp between 0-100
            
            return {
                'overall_score': round(score, 1),
                'type_check_score': max(0, 100 - type_errors * 1.5),
                'mock_density_score': max(0, 100 - mock_violations * 10),
                'grade': 'A' if score >= 90 else 'B' if score >= 80 else 'C' if score >= 70 else 'D' if score >= 60 else 'F'
            }
        
        
        def generate_markdown_report(type_check_data: Dict, mock_density_data: Dict, quality_scores: Dict) -> str:
            """Generate a comprehensive markdown report."""
            report = []
            
            report.append("# Quality Gate Report")
            report.append(f"*Generated on {datetime.now().strftime('%Y-%m-%d at %H:%M UTC')}*")
            report.append("")
            
            # Quality Score Summary
            report.append("## üìä Quality Score")
            report.append(f"**Overall Grade: {quality_scores['grade']} ({quality_scores['overall_score']}/100)**")
            report.append("")
            
            score_emoji = "üü¢" if quality_scores['overall_score'] >= 80 else "üü°" if quality_scores['overall_score'] >= 60 else "üî¥"
            report.append(f"{score_emoji} **Quality Metrics:**")
            report.append(f"- Type Safety: {quality_scores['type_check_score']:.1f}/100")
            report.append(f"- Test Quality: {quality_scores['mock_density_score']:.1f}/100")
            report.append("")
            
            # Type Checking Results
            if type_check_data:
                report.append("## üîç Type Checking")
                total_errors = type_check_data.get('total_errors', 0)
                critical_errors = type_check_data.get('critical_errors', 0)
                
                if total_errors == 0:
                    report.append("‚úÖ **No type errors found!**")
                else:
                    status = "üî¥" if critical_errors > 0 else "üü°"
                    report.append(f"{status} **{total_errors} type errors found**")
                    if critical_errors > 0:
                        report.append(f"  - üö® {critical_errors} critical errors")
                    report.append(f"  - ‚ÑπÔ∏è {total_errors - critical_errors} other errors")
                
                # Error breakdown by type
                error_types = type_check_data.get('error_types', {})
                if error_types:
                    report.append("")
                    report.append("**Error Breakdown:**")
                    for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:5]:
                        report.append(f"- {error_type}: {count}")
                        
                report.append("")
            
            # Mock Density Results  
            if mock_density_data:
                report.append("## üß™ Test Quality")
                violations = mock_density_data.get('violations', [])
                avg_density = mock_density_data.get('summary', {}).get('avg_density', 0)
                total_files = mock_density_data.get('total_files', 0)
                
                if not violations:
                    report.append("‚úÖ **Mock density within acceptable limits**")
                else:
                    report.append(f"‚ö†Ô∏è **{len(violations)} files exceed mock density thresholds**")
                
                report.append(f"- Average mock density: {avg_density:.4f}")
                report.append(f"- Test files analyzed: {total_files}")
                
                if violations:
                    report.append("")
                    report.append("**Files needing attention:**")
                    for v in violations[:5]:  # Show top 5
                        icon = "üî¥" if v.get('type') == 'new_file' else "üü°"
                        report.append(f"- {icon} `{v['file']}`: {v['density']:.4f}")
                        
                report.append("")
            
            # Recommendations
            report.append("## üéØ Recommendations")
            
            if critical_errors > 0:
                report.append("1. **üö® Critical**: Fix critical type errors immediately")
            
            if len(violations) > 0:
                report.append("2. **üìù Refactoring**: Consider migrating from MockFactory to RealComponentFactory")
                
            if quality_scores['overall_score'] < 80:
                report.append("3. **üìà Quality**: Focus on improving type safety and reducing mock dependencies")
            else:
                report.append("‚ú® **Great work!** Code quality is excellent. Keep it up!")
                
            report.append("")
            report.append("---")
            report.append("*This report was generated automatically by the Quality Gate system.*")
            
            return "\n".join(report)
        
        
        def main():
            parser = argparse.ArgumentParser(description='Generate comprehensive quality report')
            parser.add_argument('--type-check', help='Type check results JSON file')
            parser.add_argument('--mock-density', help='Mock density results JSON file') 
            parser.add_argument('--output', '-o', help='Output file path')
            parser.add_argument('--format', choices=['json', 'markdown'], default='json')
            
            args = parser.parse_args()
            
            # Load data
            type_check_data = load_json_report(args.type_check) if args.type_check else {}
            mock_density_data = load_json_report(args.mock_density) if args.mock_density else {}
            
            # Calculate quality scores
            quality_scores = calculate_quality_score(type_check_data, mock_density_data)
            
            # Generate report
            if args.format == 'markdown':
                report_content = generate_markdown_report(type_check_data, mock_density_data, quality_scores)
            else:
                report_content = json.dumps({
                    'timestamp': datetime.now().isoformat(),
                    'quality_scores': quality_scores,
                    'type_check': type_check_data,
                    'mock_density': mock_density_data
                }, indent=2)
            
            # Save or print report
            if args.output:
                with open(args.output, 'w') as f:
                    f.write(report_content)
                print(f"Report saved to {args.output}")
            else:
                print(report_content)
            
            # Exit with appropriate code based on quality
            return 0 if quality_scores['overall_score'] >= 70 else 1
        
        
        if __name__ == '__main__':
            exit(main())
        EOF
        
        chmod +x scripts/generate_quality_report.py
        
        # Generate comprehensive report
        python scripts/generate_quality_report.py \
          --type-check quality-results/type-check/type-check-report.json \
          --mock-density quality-results/mock-density/mock-density-report.json \
          --output quality-results/final/quality-report.md \
          --format markdown
          
        # Also generate JSON report for artifact storage
        python scripts/generate_quality_report.py \
          --type-check quality-results/type-check/type-check-report.json \
          --mock-density quality-results/mock-density/mock-density-report.json \
          --output quality-results/final/quality-report.json \
          --format json

    - name: Upload final quality report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: quality-gate-results
        path: quality-results/
        retention-days: 30

    - name: Post comprehensive results to PR
      uses: actions/github-script@v7
      if: github.event_name == 'pull_request' && always()
      with:
        script: |
          const fs = require('fs');
          try {
            const reportPath = 'quality-results/final/quality-report.md';
            if (!fs.existsSync(reportPath)) {
              console.log('Quality report not found, skipping PR comment');
              return;
            }
            
            const report = fs.readFileSync(reportPath, 'utf8');
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('# Quality Gate Report')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: report
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: report
              });
            }
          } catch (error) {
            console.log('Error posting quality report:', error.message);
          }

    - name: Quality gate decision
      run: |
        echo "üîç Evaluating quality gate results..."
        
        # Check individual job results
        TYPE_CHECK_RESULT="${{ needs.type-check.result }}"
        MOCK_DENSITY_RESULT="${{ needs.mock-density.result }}"
        CODE_QUALITY_RESULT="${{ needs.code-quality.result }}"
        
        echo "Type check: $TYPE_CHECK_RESULT"
        echo "Mock density: $MOCK_DENSITY_RESULT" 
        echo "Code quality: $CODE_QUALITY_RESULT"
        
        # Load quality score if available
        if [ -f "quality-results/final/quality-report.json" ]; then
          QUALITY_SCORE=$(python -c "
          import json
          try:
              with open('quality-results/final/quality-report.json') as f:
                  data = json.load(f)
              print(data.get('quality_scores', {}).get('overall_score', 0))
          except:
              print(0)
          ")
          echo "Overall quality score: $QUALITY_SCORE"
          
          if (( $(echo "$QUALITY_SCORE < 70" | bc -l) )); then
            echo "‚ùå Quality gate FAILED: Score $QUALITY_SCORE is below threshold (70)"
            exit 1
          fi
        fi
        
        # Check for critical failures
        if [[ "$TYPE_CHECK_RESULT" == "failure" ]]; then
          echo "‚ùå Quality gate FAILED: Type checking failed"
          exit 1
        fi
        
        if [[ "$CODE_QUALITY_RESULT" == "failure" ]]; then
          echo "‚ùå Quality gate FAILED: Code quality checks failed"  
          exit 1
        fi
        
        # Mock density failures are warnings, not blockers
        if [[ "$MOCK_DENSITY_RESULT" == "failure" ]]; then
          echo "‚ö†Ô∏è Warning: Mock density checks failed (not blocking)"
        fi
        
        echo "‚úÖ Quality gate PASSED: All critical checks successful"